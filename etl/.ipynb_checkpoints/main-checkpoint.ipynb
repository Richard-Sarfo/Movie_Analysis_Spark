{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a419c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2913689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from Functions.data_extraction import movie\n",
    "from Functions.Schema import get_tmdb_raw_schema\n",
    "from Functions.data_cleaning import extract_data, extract_name,get_director,separate_data\n",
    "from pyspark.sql.functions import col, lit, count , size, coalesce, when, expr, filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036898c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PySpark initialized successfully!\n",
      "   Spark Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TMDB Movie Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ PySpark initialized successfully!\")\n",
    "print(f\"   Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3819de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID 0 not found (404). Skipping.\n"
     ]
    }
   ],
   "source": [
    "#Extracting perferred movie data from the Api\n",
    "key = os.getenv(\"key\")\n",
    "\n",
    "movies = []\n",
    "\n",
    "movie_ids = [0,299534, 19995, 140607, 299536, 597, 135397, 420818,\n",
    "             24428, 168259, 99861, 284054, 12445, 181808, 330457,\n",
    "               351286, 109445, 321612, 260513\n",
    "]\n",
    "\n",
    "for movie_id in movie_ids:\n",
    "    movies.append(movie(movie_id,key))\n",
    "\n",
    "#print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37e4c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m movies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovies\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:13551\u001b[0m, in \u001b[0;36mfilter\u001b[0;34m(col, f)\u001b[0m\n\u001b[1;32m  13497\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m  13498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(\n\u001b[1;32m  13499\u001b[0m     col: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m  13500\u001b[0m     f: Union[Callable[[Column], Column], Callable[[Column, Column], Column]],\n\u001b[1;32m  13501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m  13502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13503\u001b[0m \u001b[38;5;124;03m    Returns an array of elements for which a predicate holds in a given array.\u001b[39;00m\n\u001b[1;32m  13504\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  13549\u001b[0m \u001b[38;5;124;03m    +------------------------+\u001b[39;00m\n\u001b[1;32m  13550\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_higher_order_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArrayFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:13327\u001b[0m, in \u001b[0;36m_invoke_higher_order_function\u001b[0;34m(name, cols, funs)\u001b[0m\n\u001b[1;32m  13324\u001b[0m expressions \u001b[38;5;241m=\u001b[39m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mcatalyst\u001b[38;5;241m.\u001b[39mexpressions\n\u001b[1;32m  13325\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(expressions, name)\n\u001b[0;32m> 13327\u001b[0m jcols \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m  13328\u001b[0m jfuns \u001b[38;5;241m=\u001b[39m [_create_lambda(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m funs]\n\u001b[1;32m  13330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39mColumn(expr(\u001b[38;5;241m*\u001b[39mjcols \u001b[38;5;241m+\u001b[39m jfuns)))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:13327\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m  13324\u001b[0m expressions \u001b[38;5;241m=\u001b[39m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mcatalyst\u001b[38;5;241m.\u001b[39mexpressions\n\u001b[1;32m  13325\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(expressions, name)\n\u001b[0;32m> 13327\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpr() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m  13328\u001b[0m jfuns \u001b[38;5;241m=\u001b[39m [_create_lambda(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m funs]\n\u001b[1;32m  13330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39mColumn(expr(\u001b[38;5;241m*\u001b[39mjcols \u001b[38;5;241m+\u001b[39m jfuns)))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got NoneType."
     ]
    }
   ],
   "source": [
    "movies = list(filter(None, movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2ebeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 18\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`runtime`, `credits`, `homepage`, `popularity`, `adult`].;\n'Filter isnotnull('_corrupt_record)\n+- Relation [adult#0,backdrop_path#1,belongs_to_collection#2,budget#3L,credits#4,genres#5,homepage#6,id#7L,imdb_id#8,origin_country#9,original_language#10,original_title#11,overview#12,popularity#13,poster_path#14,production_companies#15,production_countries#16,release_date#17,revenue#18L,runtime#19L,spoken_languages#20,status#21,tagline#22,title#23,... 3 more fields] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Check for corrupt records\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupt rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_corrupt_record\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNotNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Show sample data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbudget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopularity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3325\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3323\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3325\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3328\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3329\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3330\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`runtime`, `credits`, `homepage`, `popularity`, `adult`].;\n'Filter isnotnull('_corrupt_record)\n+- Relation [adult#0,backdrop_path#1,belongs_to_collection#2,budget#3L,credits#4,genres#5,homepage#6,id#7L,imdb_id#8,origin_country#9,original_language#10,original_title#11,overview#12,popularity#13,poster_path#14,production_companies#15,production_countries#16,release_date#17,revenue#18L,runtime#19L,spoken_languages#20,status#21,tagline#22,title#23,... 3 more fields] json\n"
     ]
    }
   ],
   "source": [
    "# Read with multiLine and let Spark infer schema\n",
    "df = spark.read \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .json('tmdb_movies.json')\n",
    "\n",
    "# Check for corrupt records\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Corrupt rows: {df.spark_filter(col('_corrupt_record').isNotNull()).count()}\")\n",
    "\n",
    "# Show sample data\n",
    "df.select(\"id\", \"title\", \"budget\", \"revenue\", \"popularity\").show(10, truncate=False)\n",
    "\n",
    "# Check nulls\n",
    "df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in [\"id\", \"title\", \"budget\", \"revenue\", \"credits\"]\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply extract_data function to get genre names\n",
    "df = df.withColumn(\"genres\", extract_data(col(\"genres\")))\n",
    "\n",
    "# Apply extract_name function to get collection names\n",
    "# Need to pass 'name' as a literal column since the UDF expects column parameters\n",
    "df = df.withColumn(\"belongs_to_collection\", extract_name(col(\"belongs_to_collection\"), lit(\"name\")))\n",
    "\n",
    "# Apply extract_data function to get production countries names\n",
    "df = df.withColumn(\"production_countries\", extract_data(col(\"production_countries\")))\n",
    "\n",
    "# Apply extract_data function to get production companies names\n",
    "df = df.withColumn(\"production_companies\", extract_data(col(\"production_companies\")))\n",
    "\n",
    "# Apply extract_data function to get spoken_languages\n",
    "df = df.withColumn(\"spoken_languages\", extract_data(col(\"spoken_languages\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc026053",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extracting cast from credits\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_name(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredits\u001b[39m\u001b[38;5;124m\"\u001b[39m), lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_data(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extracting crew from credits\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting cast from credits\n",
    "df = df.withColumn(\"cast\", extract_name(col(\"credits\"), lit(\"cast\")))\n",
    "df = df.withColumn(\"cast\", extract_data(col(\"cast\")))\n",
    "\n",
    "# Extracting crew from credits\n",
    "df = df.withColumn(\"crew\", extract_name(col(\"credits\"), lit(\"crew\")))\n",
    "df = df.withColumn(\"crew\", extract_data(col(\"crew\")))\n",
    "\n",
    "# Calculate cast_size and crew_size\n",
    "# size() returns the length of an array, returns -1 for null\n",
    "df = df.withColumn(\"cast_size\", when(col(\"cast\").isNotNull(), size(col(\"cast\"))).otherwise(0))\n",
    "df = df.withColumn(\"crew_size\", when(col(\"crew\").isNotNull(), size(col(\"crew\"))).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf21ea5-63ec-4698-959e-bd8a4f59de81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
